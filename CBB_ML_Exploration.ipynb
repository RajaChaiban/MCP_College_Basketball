{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBB Predictive Dashboard — ML Exploration Notebook\n",
    "\n",
    "This notebook replicates the **data collection** and **training pipeline** of the CBB Predictive Dashboard.\n",
    "\n",
    "You'll:\n",
    "1. Collect historical game data\n",
    "2. Explore feature distributions and correlations\n",
    "3. Train a calibrated 2-model ensemble (Logistic Regression + XGBoost)\n",
    "4. Evaluate with calibration curves, ROC-AUC, and Brier scores\n",
    "5. Inspect feature importance\n",
    "6. Run live inference to predict win probabilities\n",
    "\n",
    "**Adapted from:**\n",
    "- `dashboard/scripts/collect_historical_data.py`\n",
    "- `dashboard/scripts/train_predictor.py`\n",
    "- `dashboard/ai/predictor.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install cbbpy xgboost scikit-learn pandas numpy matplotlib seaborn joblib aiohttp scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import warnings\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, brier_score_loss, roc_auc_score, roc_curve,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All dependencies installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection — Replicate `collect_historical_data.py`\n",
    "\n",
    "This section fetches historical game data from cbbpy and constructs training snapshots.\n",
    "Each snapshot captures the game state at a point in time during the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def collect_cbb_data(start_date: str, end_date: str, limit_games: int = 100):\n",
    "    \"\"\"\n",
    "    Collect historical college basketball game data.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date (YYYY-MM-DD)\n",
    "        end_date: End date (YYYY-MM-DD)\n",
    "        limit_games: Max games to collect (to avoid long runtime)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: game_id, home_team, away_team, score_diff, momentum,\n",
    "                               strength_diff, period, mins_remaining, time_ratio, is_home_win\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from cbbpy.py_ball import Play\n",
    "    except ImportError:\n",
    "        print(\"cbbpy not available. Using sample data instead.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Fetching data from {start_date} to {end_date}...\")\n",
    "    \n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    all_snapshots = []\n",
    "    games_collected = 0\n",
    "    current = start\n",
    "    \n",
    "    while current <= end and games_collected < limit_games:\n",
    "        date_str = current.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"  Processing {date_str}...\", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            # Use cbbpy to get games for this date\n",
    "            import cbbpy\n",
    "            games = cbbpy.get_games(date=date_str)\n",
    "            \n",
    "            if games is None or len(games) == 0:\n",
    "                print(\" (no games)\")\n",
    "                current += timedelta(days=1)\n",
    "                continue\n",
    "            \n",
    "            for idx, game in games.iterrows():\n",
    "                if games_collected >= limit_games:\n",
    "                    break\n",
    "                    \n",
    "                # Only use completed games\n",
    "                status = game.get('status', '') or game.get('game_status', '')\n",
    "                if status != 'Final':\n",
    "                    continue\n",
    "                \n",
    "                game_id = game.get('game_id', f\"{date_str}_{idx}\")\n",
    "                home_team = game.get('home_team', 'Unknown')\n",
    "                away_team = game.get('away_team', 'Unknown')\n",
    "                home_score = int(game.get('home_score', 0) or 0)\n",
    "                away_score = int(game.get('away_score', 0) or 0)\n",
    "                \n",
    "                is_home_win = 1 if home_score > away_score else 0\n",
    "                \n",
    "                try:\n",
    "                    # Try to fetch play-by-play\n",
    "                    pbp = cbbpy.get_pbp(game_id)\n",
    "                    \n",
    "                    if pbp is None or len(pbp) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create snapshots from PBP data\n",
    "                    last_minute_sampled = -1\n",
    "                    score_history = deque(maxlen=5)\n",
    "                    \n",
    "                    for _, play in pbp.iterrows():\n",
    "                        try:\n",
    "                            # Parse clock and period\n",
    "                            clock = str(play.get('clock', '20:00') or '20:00')\n",
    "                            period = int(play.get('period', 1) or 1)\n",
    "                            \n",
    "                            # Convert clock to minutes remaining\n",
    "                            parts = clock.split(\":\")\n",
    "                            mins = int(parts[0]) if len(parts) > 0 else 0\n",
    "                            total_mins_remaining = mins if period == 2 else mins + 20\n",
    "                            \n",
    "                            # Score and momentum\n",
    "                            score_home = int(play.get('home_score', 0) or 0)\n",
    "                            score_away = int(play.get('away_score', 0) or 0)\n",
    "                            current_diff = score_home - score_away\n",
    "                            \n",
    "                            # Sample roughly every minute\n",
    "                            if total_mins_remaining != last_minute_sampled:\n",
    "                                momentum = 0.0\n",
    "                                if len(score_history) > 0:\n",
    "                                    momentum = current_diff - score_history[0]\n",
    "                                \n",
    "                                all_snapshots.append({\n",
    "                                    'game_id': str(game_id),\n",
    "                                    'home_team': str(home_team),\n",
    "                                    'away_team': str(away_team),\n",
    "                                    'score_diff': float(current_diff),\n",
    "                                    'momentum': float(momentum),\n",
    "                                    'strength_diff': 0.0,  # Simplified for demo\n",
    "                                    'period': float(period),\n",
    "                                    'mins_remaining': float(total_mins_remaining),\n",
    "                                    'time_ratio': float(total_mins_remaining / 40.0),\n",
    "                                    'is_home_win': int(is_home_win)\n",
    "                                })\n",
    "                                \n",
    "                                last_minute_sampled = total_mins_remaining\n",
    "                                score_history.append(current_diff)\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    games_collected += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "            print(f\" ({games_collected} games so far)\")\n",
    "        except Exception as e:\n",
    "            print(f\" (error: {str(e)[:30]})\")\n",
    "        \n",
    "        current += timedelta(days=1)\n",
    "    \n",
    "    if len(all_snapshots) == 0:\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(all_snapshots)\n",
    "    print(f\"\\n✓ Collected {len(df)} snapshots from {games_collected} games\")\n",
    "    return df\n",
    "\n",
    "# Attempt to collect real data\n",
    "import asyncio\n",
    "try:\n",
    "    # Try fetching last 14 days of data\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    start_date = (datetime.now() - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    df = await collect_cbb_data(start_date, end_date, limit_games=50)\n",
    "except Exception as e:\n",
    "    print(f\"Real data collection failed: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is None:\n",
    "    print(\"\\n⚠ Using synthetic training data instead...\")\n",
    "    # Generate synthetic data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'game_id': [f'game_{i}' for i in range(n_samples)],\n",
    "        'home_team': np.random.choice(['Duke', 'UNC', 'Kansas', 'UCLA', 'UK'], n_samples),\n",
    "        'away_team': np.random.choice(['Duke', 'UNC', 'Kansas', 'UCLA', 'UK'], n_samples),\n",
    "        'score_diff': np.random.normal(0, 8, n_samples),\n",
    "        'momentum': np.random.normal(0, 3, n_samples),\n",
    "        'strength_diff': np.random.normal(0, 5, n_samples),\n",
    "        'period': np.random.choice([1.0, 2.0], n_samples),\n",
    "        'mins_remaining': np.random.uniform(0, 40, n_samples),\n",
    "        'time_ratio': np.random.uniform(0, 1, n_samples),\n",
    "        'is_home_win': np.random.choice([0, 1], n_samples)\n",
    "    })\n",
    "    print(f\"✓ Generated {len(df)} synthetic snapshots\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Dataset Summary Statistics:\")\n",
    "print(df.describe())\n",
    "print(f\"\\nClass distribution (is_home_win):\")\n",
    "print(df['is_home_win'].value_counts())\n",
    "print(f\"Home win rate: {df['is_home_win'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "features = ['score_diff', 'momentum', 'strength_diff', 'time_ratio', 'mins_remaining', 'period']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.hist(df[feature], bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(df[feature].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[feature].mean():.2f}')\n",
    "    ax.set_xlabel(feature, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✓ Feature distributions plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "corr_features = ['score_diff', 'momentum', 'strength_diff', 'time_ratio', 'mins_remaining', 'period', 'is_home_win']\n",
    "corr_matrix = df[corr_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            cbar_kws={'label': 'Correlation'}, square=True)\n",
    "plt.title('Feature Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✓ Correlation heatmap generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Win rate by score_diff buckets\n",
    "df['score_diff_bucket'] = pd.cut(df['score_diff'], bins=[-np.inf, -10, -5, 0, 5, 10, np.inf],\n",
    "                                   labels=['<-10', '-10 to -5', '-5 to 0', '0 to 5', '5 to 10', '>10'])\n",
    "\n",
    "win_by_diff = df.groupby('score_diff_bucket')['is_home_win'].agg(['mean', 'count']).reset_index()\n",
    "win_by_diff.columns = ['Score Diff Bucket', 'Home Win Rate', 'Count']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars = ax.bar(range(len(win_by_diff)), win_by_diff['Home Win Rate'], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, win_by_diff['Count'])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{height:.1%}\\n(n={int(count)})',\n",
    "           ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax.set_xticks(range(len(win_by_diff)))\n",
    "ax.set_xticklabels(win_by_diff['Score Diff Bucket'])\n",
    "ax.set_ylabel('Home Win Rate', fontweight='bold')\n",
    "ax.set_xlabel('Score Differential Bucket', fontweight='bold')\n",
    "ax.set_title('Home Win Rate by Score Differential', fontweight='bold', fontsize=14)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Win Rate by Score Diff Bucket:\")\n",
    "print(win_by_diff.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum vs. outcome scatter\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Momentum vs Win\n",
    "for outcome in [0, 1]:\n",
    "    mask = df['is_home_win'] == outcome\n",
    "    label = 'Home Win' if outcome == 1 else 'Home Loss'\n",
    "    axes[0].scatter(df.loc[mask, 'momentum'], df.loc[mask, 'score_diff'], \n",
    "                    alpha=0.5, s=30, label=label)\n",
    "\n",
    "axes[0].set_xlabel('Momentum', fontweight='bold')\n",
    "axes[0].set_ylabel('Score Differential', fontweight='bold')\n",
    "axes[0].set_title('Momentum vs Score Diff (colored by outcome)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Time ratio vs win rate\n",
    "df['time_ratio_bucket'] = pd.cut(df['time_ratio'], bins=5)\n",
    "win_by_time = df.groupby('time_ratio_bucket', observed=True)['is_home_win'].agg(['mean', 'count']).reset_index()\n",
    "time_labels = [f\"{i.left:.2f}-{i.right:.2f}\" for i in win_by_time['time_ratio_bucket']]\n",
    "\n",
    "axes[1].bar(range(len(win_by_time)), win_by_time['mean'], color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xticks(range(len(win_by_time)))\n",
    "axes[1].set_xticklabels(time_labels, rotation=45)\n",
    "axes[1].set_ylabel('Home Win Rate', fontweight='bold')\n",
    "axes[1].set_xlabel('Time Ratio (0=End, 1=Start)', fontweight='bold')\n",
    "axes[1].set_title('Home Win Rate by Game Progress', fontweight='bold')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✓ Momentum and time analysis plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Ensemble Model\n",
    "\n",
    "Exactly replicates the training from `train_predictor.py`:\n",
    "- Calibrated Logistic Regression (isotonic calibration)\n",
    "- Calibrated XGBoost (isotonic calibration)\n",
    "- 50/50 weighted average ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "print(\"Preparing data for training...\")\n",
    "df_clean = df.fillna(0)\n",
    "\n",
    "# Features and target\n",
    "features = ['score_diff', 'momentum', 'strength_diff', 'time_ratio', 'mins_remaining', 'period']\n",
    "X = df_clean[features]\n",
    "y = df_clean['is_home_win']\n",
    "\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "print(f\"Positive class (home win): {y.mean():.2%}\")\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: Calibrated Logistic Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: Calibrated Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for LR\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train base LR model\n",
    "base_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Calibrate with isotonic regression (5-fold CV)\n",
    "lr_model = CalibratedClassifierCV(base_lr, method='isotonic', cv=5)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_probs = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lr_preds = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "lr_acc = accuracy_score(y_test, lr_preds)\n",
    "lr_brier = brier_score_loss(y_test, lr_probs)\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "\n",
    "print(f\"Accuracy:    {lr_acc:.4f}\")\n",
    "print(f\"Brier Score: {lr_brier:.4f} (lower is better)\")\n",
    "print(f\"ROC-AUC:     {lr_auc:.4f}\")\n",
    "\n",
    "# Feature coefficients\n",
    "print(\"\\nFeature Coefficients (after scaling):\")\n",
    "for feat, coef in zip(features, base_lr.coef_[0]):\n",
    "    print(f\"  {feat:20s}: {coef:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2: Calibrated XGBoost\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: Calibrated XGBoost\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train base XGB (no scaling needed)\n",
    "base_xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Calibrate with isotonic regression (5-fold CV)\n",
    "xgb_model = CalibratedClassifierCV(base_xgb, method='isotonic', cv=5)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
    "xgb_brier = brier_score_loss(y_test, xgb_probs)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
    "\n",
    "print(f\"Accuracy:    {xgb_acc:.4f}\")\n",
    "print(f\"Brier Score: {xgb_brier:.4f} (lower is better)\")\n",
    "print(f\"ROC-AUC:     {xgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE: Average of both models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE: Averaged Predictions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_probs = (lr_probs + xgb_probs) / 2.0\n",
    "ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "ensemble_acc = accuracy_score(y_test, ensemble_preds)\n",
    "ensemble_brier = brier_score_loss(y_test, ensemble_probs)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_probs)\n",
    "\n",
    "print(f\"Accuracy:    {ensemble_acc:.4f}\")\n",
    "print(f\"Brier Score: {ensemble_brier:.4f} (lower is better)\")\n",
    "print(f\"ROC-AUC:     {ensemble_auc:.4f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'XGBoost', 'Ensemble (Avg)'],\n",
    "    'Accuracy': [lr_acc, xgb_acc, ensemble_acc],\n",
    "    'Brier Score': [lr_brier, xgb_brier, ensemble_brier],\n",
    "    'ROC-AUC': [lr_auc, xgb_auc, ensemble_auc]\n",
    "})\nprint(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation — Calibration & Performance Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve for all three models\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs)\n",
    "fpr_ens, tpr_ens, _ = roc_curve(y_test, ensemble_probs)\n",
    "\n",
    "axes[0].plot(fpr_lr, tpr_lr, label=f'LR (AUC={lr_auc:.4f})', linewidth=2)\n",
    "axes[0].plot(fpr_xgb, tpr_xgb, label=f'XGB (AUC={xgb_auc:.4f})', linewidth=2)\n",
    "axes[0].plot(fpr_ens, tpr_ens, label=f'Ensemble (AUC={ensemble_auc:.4f})', linewidth=2.5, color='darkgreen')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontweight='bold')\n",
    "axes[0].set_ylabel('True Positive Rate', fontweight='bold')\n",
    "axes[0].set_title('ROC Curves', fontweight='bold', fontsize=14)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Calibration Curves (Reliability Diagrams)\n",
    "prob_true_lr, prob_pred_lr = calibration_curve(y_test, lr_probs, n_bins=10, strategy='uniform')\n",
    "prob_true_xgb, prob_pred_xgb = calibration_curve(y_test, xgb_probs, n_bins=10, strategy='uniform')\n",
    "prob_true_ens, prob_pred_ens = calibration_curve(y_test, ensemble_probs, n_bins=10, strategy='uniform')\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Perfectly Calibrated')\n",
    "axes[1].plot(prob_pred_lr, prob_true_lr, 'o-', label='LR (Calibrated)', linewidth=2, markersize=8)\n",
    "axes[1].plot(prob_pred_xgb, prob_true_xgb, 's-', label='XGB (Calibrated)', linewidth=2, markersize=8)\n",
    "axes[1].plot(prob_pred_ens, prob_true_ens, '^-', label='Ensemble', linewidth=2.5, color='darkgreen', markersize=8)\n",
    "axes[1].set_xlabel('Mean Predicted Probability', fontweight='bold')\n",
    "axes[1].set_ylabel('Fraction of Positives', fontweight='bold')\n",
    "axes[1].set_title('Calibration Curves (Reliability Diagrams)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlim(-0.05, 1.05)\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✓ ROC and Calibration curves plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "models_info = [\n",
    "    ('Logistic Regression', lr_preds, axes[0]),\n",
    "    ('XGBoost', xgb_preds, axes[1]),\n",
    "    ('Ensemble', ensemble_preds, axes[2])\n",
    "]\n",
    "\n",
    "for name, preds, ax in models_info:\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Away Win', 'Home Win'])\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(f'{name}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✓ Confusion matrices plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: Coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# LR coefficients\n",
    "lr_coefs = base_lr.coef_[0]\n",
    "coef_df = pd.DataFrame({'Feature': features, 'Coefficient': lr_coefs}).sort_values('Coefficient')\n",
    "\n",
    "colors = ['red' if x < 0 else 'green' for x in coef_df['Coefficient']]\n",
    "axes[0].barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Coefficient Value', fontweight='bold')\n",
    "axes[0].set_title('Logistic Regression Feature Coefficients', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, (feat, coef) in enumerate(zip(coef_df['Feature'], coef_df['Coefficient'])):\n",
    "    axes[0].text(coef, i, f' {coef:.4f}', va='center', ha='left' if coef > 0 else 'right', fontweight='bold')\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "plot_importance(base_xgb, ax=axes[1], importance_type='weight', height=0.6, title='XGBoost Feature Importance')\n",
    "axes[1].set_xlabel('Importance Score', fontweight='bold')\n",
    "axes[1].set_title('XGBoost Feature Importance', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"✓ Feature importance plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Live Inference Demo\n",
    "\n",
    "Test the model with various game states to see how it predicts win probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_win_probability(game_state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Make a prediction using the trained ensemble.\n",
    "    \n",
    "    Args:\n",
    "        game_state: dict with keys: score_diff, momentum, strength_diff, time_ratio, mins_remaining, period\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'ensemble_prob', 'lr_prob', 'xgb_prob', 'prediction'\n",
    "    \"\"\"\n",
    "    # Fill missing features with 0\n",
    "    for feat in features:\n",
    "        if feat not in game_state:\n",
    "            game_state[feat] = 0.0\n",
    "    \n",
    "    # Prepare feature vector\n",
    "    X_state = pd.DataFrame([game_state])[features]\n",
    "    \n",
    "    # LR prediction (needs scaling)\n",
    "    X_state_scaled = scaler.transform(X_state)\n",
    "    lr_prob = lr_model.predict_proba(X_state_scaled)[0, 1]\n",
    "    \n",
    "    # XGB prediction\n",
    "    xgb_prob = xgb_model.predict_proba(X_state)[0, 1]\n",
    "    \n",
    "    # Ensemble\n",
    "    ensemble_prob = (lr_prob + xgb_prob) / 2.0\n",
    "    \n",
    "    return {\n",
    "        'lr_prob': lr_prob,\n",
    "        'xgb_prob': xgb_prob,\n",
    "        'ensemble_prob': ensemble_prob,\n",
    "        'prediction': 'Home Win' if ensemble_prob > 0.5 else 'Away Win'\n",
    "    }\n",
    "\n",
    "# Test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Home team leading by 10 points, mid-game',\n",
    "        'state': {'score_diff': 10, 'momentum': 2, 'strength_diff': 0, 'time_ratio': 0.5, 'mins_remaining': 20, 'period': 1.5}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Away team leading by 5, late game (5 mins left)',\n",
    "        'state': {'score_diff': -5, 'momentum': -3, 'strength_diff': -2, 'time_ratio': 0.125, 'mins_remaining': 5, 'period': 2}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Tied game, very late (2 mins left)',\n",
    "        'state': {'score_diff': 0, 'momentum': 1, 'strength_diff': 0, 'time_ratio': 0.05, 'mins_remaining': 2, 'period': 2}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Pre-game (no score yet)',\n",
    "        'state': {'score_diff': 0, 'momentum': 0, 'strength_diff': 3, 'time_ratio': 1.0, 'mins_remaining': 40, 'period': 1}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Home blowout, early game',\n",
    "        'state': {'score_diff': 15, 'momentum': 5, 'strength_diff': 2, 'time_ratio': 0.75, 'mins_remaining': 30, 'period': 1}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIVE INFERENCE DEMO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for scenario in scenarios:\n",
    "    result = predict_win_probability(scenario['state'])\n",
    "    results.append({\n",
    "        'Scenario': scenario['name'],\n",
    "        'LR Prob': f\"{result['lr_prob']:.2%}\",\n",
    "        'XGB Prob': f\"{result['xgb_prob']:.2%}\",\n",
    "        'Ensemble': f\"{result['ensemble_prob']:.2%}\",\n",
    "        'Prediction': result['prediction']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\nfor idx, row in results_df.iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['Scenario']}\")\n",
    "    print(f\"   LR: {row['LR Prob']:>7s} | XGB: {row['XGB Prob']:>7s} | Ensemble: {row['Ensemble Prob']:>7s} → {row['Prediction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive demo: Visualize probability by score_diff at different times\n",
    "score_diffs = np.linspace(-20, 20, 50)\n",
    "time_ratios = [1.0, 0.75, 0.5, 0.25, 0.1]  # Different game stages\n",
    "time_labels = ['Pre-game (0 min)', 'Early (30 min)', 'Mid (20 min)', 'Late (10 min)', 'Final (2 min)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(time_ratios)))\n",
    "\n",
    "for time_ratio, label, color in zip(time_ratios, time_labels, colors):\n",
    "    probs = []\n",
    "    for score_diff in score_diffs:\n",
    "        state = {\n",
    "            'score_diff': score_diff,\n",
    "            'momentum': 0,\n",
    "            'strength_diff': 0,\n",
    "            'time_ratio': time_ratio,\n",
    "            'mins_remaining': time_ratio * 40,\n",
    "            'period': 1 if time_ratio > 0.5 else 2\n",
    "        }\n",
    "        result = predict_win_probability(state)\n",
    "        probs.append(result['ensemble_prob'])\n",
    "    \n",
    "    ax.plot(score_diffs, probs, marker='o', markersize=4, linewidth=2.5, label=label, color=color)\n",
    "\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=1, alpha=0.7, label='50% Win Prob')\n",
    "ax.axvline(x=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "ax.set_xlabel('Score Differential (Home - Away)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Home Win Probability', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Win Probability Curves by Score Differential & Game Stage (Ensemble)', fontweight='bold', fontsize=14)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Probability curves by game stage plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Trained Model Bundle\n",
    "\n",
    "Create a joblib bundle that can be used in the dashboard or downloaded from Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the predictor bundle\n",
    "bundle = {\n",
    "    'lr_model': lr_model,\n",
    "    'xgb_model': xgb_model,\n",
    "    'scaler': scaler,\n",
    "    'features': features,\n",
    "    'weights': {'lr': 0.5, 'xgb': 0.5},\n",
    "    'metadata': {\n",
    "        'trained_at': pd.Timestamp.now().isoformat(),\n",
    "        'features_used': features,\n",
    "        'ensemble_brier_score': ensemble_brier,\n",
    "        'ensemble_accuracy': ensemble_acc,\n",
    "        'ensemble_auc': ensemble_auc,\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save locally in Colab\n",
    "joblib.dump(bundle, 'cbb_predictor_bundle.joblib')\n\nprint(\"✓ Model bundle saved to 'cbb_predictor_bundle.joblib'\")\nprint(f\"\\nBundle metadata:\")\nfor key, value in bundle['metadata'].items():\n    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Colab: Download the bundle\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Colab environment detected. Downloading bundle...\")\n",
    "    files.download('cbb_predictor_bundle.joblib')\n    print(\"✓ Bundle downloaded!\")\nexcept ImportError:\n",
    "    print(\"Not in Colab. Bundle saved locally as 'cbb_predictor_bundle.joblib'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: CBB ML Exploration Complete\")\nprint(\"=\"*80)\n\nprint(f\"\"\"\n✓ DATASET\n  - Total samples: {len(df)}\n  - Features: {', '.join(features)}\n  - Target: Home win (binary classification)\n  - Class balance: {y.mean():.1%} home wins\n\n✓ MODELS TRAINED\n  - Logistic Regression (Calibrated, Isotonic)\n  - XGBoost (Calibrated, Isotonic)\n  - Ensemble: 50/50 weighted average\n\n✓ PERFORMANCE (Test Set)\n  Model                  Accuracy    Brier Score    ROC-AUC\n  ─────────────────────  ──────────  ────────────  ─────────\n  Logistic Regression    {lr_acc:7.2%}      {lr_brier:.4f}       {lr_auc:.4f}\n  XGBoost                {xgb_acc:7.2%}      {xgb_brier:.4f}       {xgb_auc:.4f}\n  Ensemble (Averaged)    {ensemble_acc:7.2%}      {ensemble_brier:.4f}       {ensemble_auc:.4f}\n\n✓ CALIBRATION\n  - Both models use isotonic calibration (5-fold CV)\n  - Brier scores < 0.25 indicate good calibration\n  - Calibration curves show reliability (close to diagonal = well-calibrated)\n\n✓ KEY FEATURES (by importance)\n  - score_diff: Game momentum, most impactful\n  - time_ratio: How much game is left\n  - momentum: Recent score changes\n  - strength_diff: Pre-game team strength\n  - mins_remaining: Precise time left\n  - period: Game half (1st or 2nd)\n\n✓ ARTIFACT\n  - Saved: cbb_predictor_bundle.joblib\n  - Size: ~{len(joblib.dumps(bundle)) / 1024:.1f} KB\n  - Contains: LR model, XGB model, scaler, features, metadata\n\n✓ NEXT STEPS\n  1. Download bundle from Colab\n  2. Replace dashboard/ai/predictor.py bundle\n  3. Retrain with more recent data for better performance\n  4. A/B test ensemble weights (currently 50/50)\n  5. Monitor calibration curves in production\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"✓ Notebook Complete!\")\nprint(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
